import base64
from io import BytesIO
from typing import List, Tuple

import tiktoken
from openai import OpenAI
from PIL import Image

from llm.llm_service import LLMService


class OpenAIProvider(LLMService):

    GPT_3_5_TURBO = "gpt-3.5-turbo"
    GPT_4_TURBO = "gpt-4-turbo"

    _MAX_INPUT_TOKENS = "max_input_tokens"
    _MAX_OUTPUT_TOKENS = "max_output_tokens"
    _IS_VISION = "is_vision"
    _COST_PER_INPUT_TOKEN = "cost_per_input_token"
    _COST_PER_OUTPUT_TOKEN = "cost_per_output_token"
    _COST_PER_INPUT_IMAGE = "cost_per_input_image"

    MODEL_PARAMETERS = {
        GPT_3_5_TURBO: {
            _MAX_INPUT_TOKENS: 16385,
            _IS_VISION: False,
            _COST_PER_INPUT_TOKEN: .0000005,
            _COST_PER_OUTPUT_TOKEN: .0000015,
        },
        GPT_4_TURBO: {
            _MAX_INPUT_TOKENS: 128000,
            _IS_VISION: True,
            _COST_PER_INPUT_TOKEN: .00001,
            _COST_PER_OUTPUT_TOKEN: .00003,
            _COST_PER_INPUT_IMAGE: .00255  # TODO: refine based on the image size
        }
    }

    def __init__(self, openai_api_key: str, model_name: str, max_output_tokens: int = 500):
        """
        Initializes the OpenAIProvider with the given API key, model name, and maximum output tokens.

        Args:
            openai_api_key (str): The API key for authenticating with the OpenAI service.
            model_name (str): The name of the model to use (e.g., "gpt-3.5-turbo").
            max_output_tokens (int, optional): The maximum number of tokens for the output. Defaults to 500.
        """
        self.model_name = model_name
        self.max_output_tokens = max_output_tokens
        self.client = OpenAI(api_key=openai_api_key)
        self.tokenizer = tiktoken.encoding_for_model(model_name)

    def get_max_input_tokens(self) -> int:
        """
        Retrieve the maximum number of input tokens allowed for the current model.
        """
        return self.MODEL_PARAMETERS[self.model_name][self._MAX_INPUT_TOKENS]

    def is_vision(self) -> bool:
        """
        Determine if the current model supports vision features.
        """
        return self.MODEL_PARAMETERS[self.model_name][self._IS_VISION]

    def get_cost_per_input_token(self) -> float:
        """
        Retrieves the cost per input token for the current model.
        """
        return self.MODEL_PARAMETERS[self.model_name][self._COST_PER_INPUT_TOKEN]

    def get_cost_per_output_token(self) -> float:
        """
        Retrieves the cost per output token for the current model.
        """
        return self.MODEL_PARAMETERS[self.model_name][self._COST_PER_OUTPUT_TOKEN]

    def get_cost_per_input_image(self) -> float:
        """
        Retrieves the cost per input image for the current model.
        """
        return self.MODEL_PARAMETERS[self.model_name][self._COST_PER_INPUT_IMAGE]

    def get_provider_name(self) -> str:
        return f"OpenAI_{self.model_name}"

    def count_tokens(self, prompt) -> int:
        """
        Counts the number of tokens in the given prompt.

        Args:
            prompt (str): The input prompt string.

        Returns:
            int: The number of tokens in the prompt.
        """
        return len(self.tokenizer.encode(prompt))

    def trim_prompt(self, prompt: str) -> str:
        # Encode
        tokens = self.tokenizer.encode(prompt)

        # Trim
        if len(tokens) > self.get_max_input_tokens():
            n_extra_tokens = len(tokens) - self.get_max_input_tokens()
            self._info(f"Prompt too long ({len(tokens)} t), removing {
                       n_extra_tokens} t")
            tokens = tokens[:n_extra_tokens]

        # Decode
        trimmed_prompt = self.tokenizer.decode(tokens)
        return trimmed_prompt

    def calculate_cost(self, input: str, output: str, images: list) -> float:
        """
        Calculates the total cost of the LLM call based on the input text, output text, and number of images.

        Args:
            input (str): The input text string.
            output (str): The output text string generated by the LLM.
            images (list): A list of images associated with the LLM call.

        Returns:
            float: The total cost of the LLM call.
        """
        input_tokens = self.tokenizer.encode(input)
        output_tokens = self.tokenizer.encode(output)

        return len(input_tokens) * self.get_cost_per_input_token() + \
            len(output_tokens) * self.get_cost_per_output_token() + \
            len(images) * self.get_cost_per_input_image()

    def generate_text(self, prompt: str) -> Tuple[str, float]:
        prompt = self.trim_prompt(prompt)

        response = self.client.chat.completions.create(
            messages=[{"role": "system", "content": prompt}],
            model=self.model_name
        )
        response_text = response.choices[0].message.content

        response_cost = self.calculate_cost(input=prompt,
                                            output=response_text,
                                            images=[])
        print(f"cost: {response_cost}$")

        return response_text, response_cost

    def generate_text_with_images(self, prompt: str, pil_images: List[Image.Image]) -> Tuple[str, float]:
        """
        Generates text based on a prompt and a list of PIL images.

        Args:
            prompt (str): The prompt for generating the text.
            pil_images (List[PIL.Image.Image]): A list of PIL Image objects.
            use_base64 (bool): Flag to determine if images should be passed as base64 encoded strings.

        Returns:
            str: The generated text or an error message.
        """
        if not self.is_vision():
            raise NotImplementedError(
                f"Model {self.model_name} doesn't accept images")

        messages = [{"role": "user", "content": [
            {"type": "text", "text": prompt}]}]

        for image in pil_images:
            buffered = BytesIO()
            image.save(buffered, format="JPEG")
            encoded_image = base64.b64encode(
                buffered.getvalue()).decode("utf-8")
            image_content = {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            }
            messages[0]["content"].append(image_content)

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=self.max_output_tokens
        )
        response_text = response.choices[0].message.content

        response_cost = self.calculate_cost(input=prompt,
                                            output=response_text, images=pil_images)
        print(f"cost: {response_cost}$")

        return response_text, response_cost

    def chat(self, initial_prompt_text: str, conversation_history: list, user_input: str):
        """
        TODO
        """
        if len(conversation_history) == 0:  # conversation just started, include initial_prompt as system prompt
            conversation_history.append(
                {"role": "system", "content": initial_prompt_text})

        if user_input is not None:
            conversation_history.append(
                {"role": "user", "content": user_input}
            )

        response = self.client.chat.completions.create(
            messages=conversation_history,
            model=self.model_name
        )

        assistant_response = response.choices[0].message.content
        conversation_history.append(
            {"role": "assistant", "content": assistant_response})

        return conversation_history, assistant_response
